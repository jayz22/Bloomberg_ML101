
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[8pt]{extarticle}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{iris\_lab}
    
    
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=0.5in,bmargin=0.5in,lmargin=0.2in,rmargin=0.2in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{ML1 Lab}\label{ml1-lab}

    Welcome to ML1! This lab will use the famous Iris data set to introduce
you to common python ML tools. You will explore, analyze, and then build
and tune models to predict flower species. Click
\href{https://en.wikipedia.org/wiki/Iris_flower_data_set}{here} for more
about the Iris dataset.

    \subsection{About the tools}\label{about-the-tools}

    Scientific python uses a set of libraries that build on one another.
Here's a one-line intro to each, from lowest level to highest.

\begin{itemize}
\tightlist
\item
  \textbf{NumPy}: fast vector and matrix math
\item
  \textbf{pandas}: Tools to read/write/manipulate data with a structure
  called the DataFrame. A bit like Excel.
\item
  \textbf{SciPy}: scientific computation code,like Linear Regression and
  sparse matrices
\item
  \textbf{scikit-learn}: ML algorithms and helper routines, such as
  feature extraction
\item
  \textbf{Matplotlib}: graphing stuff!
\item
  \textbf{Jupyter notebooks}: the web-based UI you are using now that
  allows text, code, and output to be shared,edited, and viewed.
\end{itemize}

    \subsection{Getting Around}\label{getting-around}

    Not familar with notebooks? Think of it as a fancy word processor. Each
section is a cell that can contain text or code. When code is run,
output is shown below. A good introductory tutorial can be found
\href{https://www.dataquest.io/blog/jupyter-notebook-tutorial}{here}.

Now, let's get started!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k+kn}{as} \PY{n+nn}{pd}
         \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
\end{Verbatim}

    \subsection{Loading data}\label{loading-data}

    To analyze data, we are going to need to import it from an external
source first! Let's use the popular, high-level library \texttt{pandas}
to load the Iris data csv as a \texttt{DataFrame}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{n}{iris\PYZus{}csv\PYZus{}path} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./data/iris.data}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{iris\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{n}{iris\PYZus{}csv\PYZus{}path}\PY{p}{)}
\end{Verbatim}

    \texttt{DataFrames} are tabular data structures; let's sample a few rows
at random to get a preliminary sense of our table:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{sample}\PY{p}{(}\PY{n}{n}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}47}]:}      sepal\_length  sepal\_width  petal\_length  petal\_width            class
         28            5.2          3.4           1.4          0.2      Iris-setosa
         38            4.4          3.0           1.3          0.2      Iris-setosa
         19            5.1          3.8           1.5          0.3      Iris-setosa
         88            5.6          3.0           4.1          1.3  Iris-versicolor
         137           6.4          3.1           5.5          1.8   Iris-virginica
\end{Verbatim}
            
    Here are two observations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The columns of a \texttt{DataFrame} need not be the same data-type
  (\texttt{petal\_width} is a \texttt{float}, while \texttt{class} is a
  \texttt{string}); we can think of a \texttt{DataFrame} as being a
  dictionary that maps column labels to \texttt{pandas} \texttt{Series}
  objects, which are one-dimensional arrays of data. We'll explore the
  relationship between \texttt{DataFrames} and \texttt{Series} a little
  more deeply in a few steps!
\item
  All \texttt{pandas} Data Structures are "labeled", which we can see by
  values of the \texttt{index} (i.e. what looks to be the left-most
  column). This would come in handy when merging \texttt{Series} of
  end-of-day prices for various securities, for example, since
  \texttt{pandas} would line up the dates for us (see the
  \textbf{Appendix} for an example of this!)
\end{enumerate}

    \subsection{Exploratory Data Analysis}\label{exploratory-data-analysis}

    The purpose of this lab is to use the features \texttt{sepal\_length},
\texttt{sepal\_width}, \texttt{petal\_length} and \texttt{petal\_width}
to predict the correct class of Iris (e.g. \texttt{Iris-versicolor}) for
each of the examples that we've been provided. Before we dive into
visualizations and machine learning, however, let's spend a few minutes
getting more familiar with the data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{describe}\PY{p}{(}\PY{n}{include}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{all}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}48}]:}         sepal\_length  sepal\_width  petal\_length  petal\_width        class
         count     150.000000   150.000000    150.000000   150.000000          150
         unique           NaN          NaN           NaN          NaN            3
         top              NaN          NaN           NaN          NaN  Iris-setosa
         freq             NaN          NaN           NaN          NaN           50
         mean        5.843333     3.054000      3.758667     1.198667          NaN
         std         0.828066     0.433594      1.764420     0.763161          NaN
         min         4.300000     2.000000      1.000000     0.100000          NaN
         25\%         5.100000     2.800000      1.600000     0.300000          NaN
         50\%         5.800000     3.000000      4.350000     1.300000          NaN
         75\%         6.400000     3.300000      5.100000     1.800000          NaN
         max         7.900000     4.400000      6.900000     2.500000          NaN
\end{Verbatim}
            
    We can see the mean for each feature across all of the samples, but are
these consistent across Iris classes?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}49}]:}                  sepal\_length  sepal\_width  petal\_length  petal\_width
         class                                                                
         Iris-setosa             5.006        3.418         1.464        0.244
         Iris-versicolor         5.936        2.770         4.260        1.326
         Iris-virginica          6.588        2.974         5.552        2.026
\end{Verbatim}
            
    While 4 features is pretty manageable, suppose we were provided with
hundreds of columns but only a few were of interest. We can sub-select
columns as follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{n}{iris\PYZus{}df}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}50}]:}    sepal\_length  sepal\_width
         0           5.1          3.5
         1           4.9          3.0
         2           4.7          3.2
         3           4.6          3.1
         4           5.0          3.6
\end{Verbatim}
            
    And notice that if we select only one column, we get a \texttt{pandas}
\texttt{Series} object back:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{n}{sepal\PYZus{}length} \PY{o}{=} \PY{n}{iris\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{k}{print}\PY{p}{(}\PY{n+nb}{type}\PY{p}{(}\PY{n}{sepal\PYZus{}length}\PY{p}{)}\PY{p}{)}
         \PY{n}{sepal\PYZus{}length}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
<class 'pandas.core.series.Series'>

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}51}]:} 0    5.1
         1    4.9
         2    4.7
         3    4.6
         4    5.0
         Name: sepal\_length, dtype: float64
\end{Verbatim}
            
    And this \texttt{Series} can easily converted back to a 1-column
\texttt{DataFrame}:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{n}{sepal\PYZus{}length}\PY{o}{.}\PY{n}{to\PYZus{}frame}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}52}]:}    sepal\_length
         0           5.1
         1           4.9
         2           4.7
         3           4.6
         4           5.0
\end{Verbatim}
            
    Now that we've been exposed to a few of these methods, let's combine
them to check whether any Iris classes are more heavily represented than
the others

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{c+c1}{\PYZsh{} .size() returns a Series}
         \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to\PYZus{}frame}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{counts}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}53}]:}                  counts
         class                  
         Iris-setosa          50
         Iris-versicolor      50
         Iris-virginica       50
\end{Verbatim}
            
    Suppose we now want to examine a subset of \emph{rows} in our
\texttt{DataFrame}, and we want to select them based on their column
values (similar to SQL):

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{query}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}length \PYZgt{} 7}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}54}]:}      sepal\_length  sepal\_width  petal\_length  petal\_width           class
         102           7.1          3.0           5.9          2.1  Iris-virginica
         105           7.6          3.0           6.6          2.1  Iris-virginica
         107           7.3          2.9           6.3          1.8  Iris-virginica
         109           7.2          3.6           6.1          2.5  Iris-virginica
         117           7.7          3.8           6.7          2.2  Iris-virginica
         118           7.7          2.6           6.9          2.3  Iris-virginica
         122           7.7          2.8           6.7          2.0  Iris-virginica
         125           7.2          3.2           6.0          1.8  Iris-virginica
         129           7.2          3.0           5.8          1.6  Iris-virginica
         130           7.4          2.8           6.1          1.9  Iris-virginica
         131           7.9          3.8           6.4          2.0  Iris-virginica
         135           7.7          3.0           6.1          2.3  Iris-virginica
\end{Verbatim}
            
    While \texttt{pandas} has become the tool of choice for most data
science tasks, \texttt{numpy} is a lower-level (but also very popular)
Python library that comes in handy when doing work with vectors and
matrices, or implementing machine learning algorithms from scratch.
Luckily, it plays very well with \texttt{pandas} and pure Python. For
our purposes, let's just explore a few common use-cases.

Imagine we want to use a machine learning library that expects us to
represent our observations as a 2-dimensional \texttt{numpy} array
instead of a \texttt{DataFrame}. We can easily convert our
\texttt{DataFrame} to this format as follows:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} \PY{c+c1}{\PYZsh{} first drop the class labels, since **all values in a numpy array must have the same data\PYZhy{}type**}
         \PY{n}{iris\PYZus{}features\PYZus{}df} \PY{o}{=} \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} The call to .values does the magic of converting our DataFrame to a numpy array. This works for Series objects, too!}
         \PY{n}{iris\PYZus{}features\PYZus{}ndarray} \PY{o}{=} \PY{n}{iris\PYZus{}features\PYZus{}df}\PY{o}{.}\PY{n}{values}
         \PY{n}{iris\PYZus{}features\PYZus{}ndarray}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{5}\PY{p}{]}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}55}]:} array([[5.1, 3.5, 1.4, 0.2],
                [4.9, 3. , 1.4, 0.2],
                [4.7, 3.2, 1.3, 0.2],
                [4.6, 3.1, 1.5, 0.2],
                [5. , 3.6, 1.4, 0.2]])
\end{Verbatim}
            
    To go full circle, let's convert it back!

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{iris\PYZus{}features\PYZus{}ndarray}\PY{p}{,} \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}56}]:}    sepal\_length  sepal\_width  petal\_length  petal\_width
         0           5.1          3.5           1.4          0.2
         1           4.9          3.0           1.4          0.2
         2           4.7          3.2           1.3          0.2
         3           4.6          3.1           1.5          0.2
         4           5.0          3.6           1.4          0.2
\end{Verbatim}
            
    Here are some commonly-used \texttt{numpy} functions, which we'll use
when tuning our model later in this lab

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}57}]:} \PY{c+c1}{\PYZsh{} Create an evenly spaced array}
         \PY{k}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Convert a plain Python list to an array}
         \PY{k}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{n}{i} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[0 1 2 3 4]
[  0.1   1.   10.  100. ]

    \end{Verbatim}

    Please check out the \textbf{Appendix} at the bottom of this notebook
for more advanced material on \texttt{pandas} and \texttt{numpy}

    \section{Data visualization}\label{data-visualization}

In this section we will use the module \texttt{matplotlib.pyplot}, a
MATLAB-like plotting framework.

\href{http://nbviewer.jupyter.org/github/matplotlib/AnatomyOfMatplotlib/blob/master/AnatomyOfMatplotlib-Part1-Figures_Subplots_and_layouts.ipynb}{Overview
of matplotlib, with links to documentation and tutorials}

First of all, let's configure matplotlib so that it works fine in your
environment.

\textbf{Important:} Remember to set \texttt{in\_bqnt} to True if you are
in BQNT \textless{}GO\textgreater{}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}58}]:} \PY{k+kn}{import} \PY{n+nn}{matplotlib.pyplot} \PY{k+kn}{as} \PY{n+nn}{plt}
         
         \PY{c+c1}{\PYZsh{} configure the notebook to display the figures inline}
         \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
         
         \PY{c+c1}{\PYZsh{} is this running in BQNT?}
         \PY{n}{in\PYZus{}bqnt} \PY{o}{=} \PY{n+nb+bp}{False}
         
         \PY{c+c1}{\PYZsh{} set up light coloring for BQNT\PYZsq{}s dark background}
         \PY{k}{if} \PY{n}{in\PYZus{}bqnt}\PY{p}{:}
             \PY{n}{rc\PYZus{}params} \PY{o}{=} \PY{p}{\PYZob{}}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{xtick}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{color}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{white}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ytick}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{color}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{white}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}\PY{p}{,}
                 \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{axes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{labelcolor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{white}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}}
             \PY{p}{\PYZcb{}}
             \PY{k}{for} \PY{n}{group}\PY{p}{,} \PY{n}{args} \PY{o+ow}{in} \PY{n}{rc\PYZus{}params}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                 \PY{n}{plt}\PY{o}{.}\PY{n}{rc}\PY{p}{(}\PY{n}{group}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{args}\PY{p}{)}
\end{Verbatim}

    \subsection{Feature Histogram}\label{feature-histogram}

This section covers:

\begin{itemize}
\tightlist
\item
  Creating a histogram plot
\item
  Adding axis labels
\item
  Using a new figure for each plot
\end{itemize}

    \subsubsection{Histogram}\label{histogram}

Just pass one-dimensional data (and optionally the number of bins) to
the \texttt{plt.hist} function to draw a histogram.

To add a label to the \(x\) and \(y\) axes, you can use
\texttt{plt.xlabel} and \texttt{plt.ylabel}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}59}]:} \PY{k}{def} \PY{n+nf}{draw\PYZus{}histogram}\PY{p}{(}\PY{n}{feature\PYZus{}values}\PY{p}{,} \PY{n}{feature\PYZus{}name}\PY{p}{)}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{feature\PYZus{}values}\PY{p}{,} \PY{n}{bins}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)} \PY{c+c1}{\PYZsh{} draw a histogram of feature\PYZus{}values with 10 bins}
             \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{feature\PYZus{}name}\PY{p}{)}  \PY{c+c1}{\PYZsh{} set the x label to feature\PYZus{}name}
             \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}  \PY{c+c1}{\PYZsh{} set the y label to \PYZsq{}count\PYZsq{}}
         
         \PY{n}{feature\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}  \PY{c+c1}{\PYZsh{} or iris\PYZus{}df.columns.values[:\PYZhy{}1]}
         \PY{k}{for} \PY{n}{feature\PYZus{}name} \PY{o+ow}{in} \PY{n}{feature\PYZus{}names}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} this line makes each histogram use a different figure, try removing it to see what happens!}
             \PY{c+c1}{\PYZsh{}feature\PYZus{}values = ...  \PYZsh{} get the data for a given feature\PYZus{}name}
             \PY{n}{feature\PYZus{}values} \PY{o}{=} \PY{n}{iris\PYZus{}df}\PY{p}{[}\PY{n}{feature\PYZus{}name}\PY{p}{]}
             \PY{n}{draw\PYZus{}histogram}\PY{p}{(}\PY{n}{feature\PYZus{}values}\PY{p}{,} \PY{n}{feature\PYZus{}name}\PY{p}{)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{iris_lab_files/iris_lab_39_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{iris_lab_files/iris_lab_39_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{iris_lab_files/iris_lab_39_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{iris_lab_files/iris_lab_39_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Separating dataset by a
value}\label{separating-dataset-by-a-value}

In this classification problem, we want to find the species of a given
flower given only the measurements.

When looking at the petal length and petal width histogram, we see that
there seems to be two peaks (around 1.5 and around 5).

Are these peaks a good way to distinguish species? Why do we have two
peaks even though we have three possible species? A good way to answer
these questions is to separate the dataset by classes, to see how each
class is influencing the shape of the histogram.

This section covers:

\begin{itemize}
\tightlist
\item
  Separating the dataset by the value of a feature ('class' in this
  case)
\item
  Adding multiple data series to the same plot
\item
  Adding a legend to a plot
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{n}{classes} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iris\PYZhy{}setosa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iris\PYZhy{}versicolor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iris\PYZhy{}virginica}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}  \PY{c+c1}{\PYZsh{} or iris\PYZus{}df[\PYZsq{}class\PYZsq{}].unique()}
         \PY{n}{feature\PYZus{}name} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}
         
         \PY{n}{true\PYZus{}if\PYZus{}setosa} \PY{o}{=} \PY{n}{iris\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iris\PYZhy{}setosa}\PY{l+s+s1}{\PYZsq{}}
         \PY{n}{only\PYZus{}setosa\PYZus{}df} \PY{o}{=} \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{true\PYZus{}if\PYZus{}setosa}\PY{p}{]}
         \PY{n}{feature\PYZus{}values} \PY{o}{=} \PY{n}{only\PYZus{}setosa\PYZus{}df}\PY{p}{[}\PY{n}{feature\PYZus{}name}\PY{p}{]}
         \PY{k}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iris\PYZhy{}setosa \PYZob{}\PYZcb{} average: \PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{feature\PYZus{}name}\PY{p}{,} \PY{n}{feature\PYZus{}values}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Iris-setosa petal\_length average: 1.464

    \end{Verbatim}

    The \texttt{Iris-setosa} average is indeed around 1.5!

But what are the minimum and maximum values? And how about the other
classes?

\subsubsection{Exercise 1. Draw a separated feature histogram for each
feature}\label{exercise-1.-draw-a-separated-feature-histogram-for-each-feature}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  select \textbf{only} the feature values corresponding to \texttt{cls}
  in \texttt{feature\_values}
\item
  look at the histogram with all three classes
\item
  Plot the separated histogram of each feature
\item
  Make sure that the \(x\) label and \(y\) label are set
\item
  Also make sure the legend is visible
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}61}]:} \PY{k}{def} \PY{n+nf}{only\PYZus{}cls\PYZus{}feature\PYZus{}values}\PY{p}{(}\PY{n}{input\PYZus{}df}\PY{p}{,} \PY{n+nb+bp}{cls}\PY{p}{,} \PY{n}{feature\PYZus{}name}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{n}{input\PYZus{}df}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{n}{input\PYZus{}df}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{==} \PY{n+nb+bp}{cls}\PY{p}{]}\PY{p}{[}\PY{n}{feature\PYZus{}name}\PY{p}{]}
         
         \PY{k}{for} \PY{n}{feature\PYZus{}name} \PY{o+ow}{in} \PY{n}{feature\PYZus{}names}\PY{p}{:}
             \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} add plots to a new figure}
             \PY{k}{for} \PY{n+nb+bp}{cls} \PY{o+ow}{in} \PY{n}{classes}\PY{p}{:}
                 \PY{n}{cls\PYZus{}fv} \PY{o}{=} \PY{n}{only\PYZus{}cls\PYZus{}feature\PYZus{}values}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{n+nb+bp}{cls}\PY{p}{,} \PY{n}{feature\PYZus{}name}\PY{p}{)}
                 \PY{n}{draw\PYZus{}histogram}\PY{p}{(}\PY{n}{cls\PYZus{}fv}\PY{p}{,} \PY{n}{feature\PYZus{}name}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{classes}\PY{p}{)}
             \PY{c+c1}{\PYZsh{}plt.title(feature\PYZus{}name)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{iris_lab_files/iris_lab_43_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{iris_lab_files/iris_lab_43_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{iris_lab_files/iris_lab_43_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{iris_lab_files/iris_lab_43_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Scatterplots of feature
pairs}\label{scatterplots-of-feature-pairs}

When analyzing multi dimensional data, it is sometimes interesting to
see how some dimensions interact with each other. In order to do this,
one possible tool is a scatter plot.

This section covers:

\begin{itemize}
\tightlist
\item
  Creating a scatter plot of a pair of features
\item
  Interpreting the plot
\end{itemize}

    \subsubsection{Scatterplots}\label{scatterplots}

The way to plot scatter plots with \texttt{matplotlib.pyplot} is the
same as a line plot, except that we need to provide one more argument at
the end, for the shape.

Here is a simple example, of the square function, using
\texttt{\textquotesingle{}.\textquotesingle{}} instead of
\texttt{\textquotesingle{}-\textquotesingle{}} (the default).

Other possible values include
\texttt{\textquotesingle{}x\textquotesingle{}},
\texttt{\textquotesingle{}+\textquotesingle{}}, etc. (see
https://matplotlib.org/api/pyplot\_api.html\#matplotlib.pyplot.plot for
a full list).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}62}]:} \PY{k}{def} \PY{n+nf}{square}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{n}{x} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}
         
         \PY{n}{x\PYZus{}values} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{num}\PY{o}{=}\PY{l+m+mi}{11}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}values}\PY{p}{,} \PY{n}{square}\PY{p}{(}\PY{n}{x\PYZus{}values}\PY{p}{)}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{iris_lab_files/iris_lab_46_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Exercise 2. Plot a pair of
features}\label{exercise-2.-plot-a-pair-of-features}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Get the \texttt{x\_values} for a given class and the
  \texttt{x\_feature} feature name
\item
  Same thing for \texttt{y\_values}
\end{enumerate}

(hint: use \texttt{only\_cls\_feature\_values}) 3. Plot the
\texttt{x\_values} against \texttt{y\_values} with a the
\texttt{\textquotesingle{}.\textquotesingle{}} shape

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}63}]:} \PY{k}{def} \PY{n+nf}{scatter\PYZus{}plot\PYZus{}feature\PYZus{}pair}\PY{p}{(}\PY{n}{input\PYZus{}df}\PY{p}{,} \PY{n}{x\PYZus{}feature}\PY{p}{,} \PY{n}{y\PYZus{}feature}\PY{p}{,} \PY{n}{classes}\PY{p}{,} \PY{n}{plt}\PY{o}{=}\PY{n}{plt}\PY{p}{)}\PY{p}{:}
             \PY{n}{handles} \PY{o}{=} \PY{p}{[}\PY{p}{]}  \PY{c+c1}{\PYZsh{} used for plt.figlegend}
         
             \PY{k}{for} \PY{n+nb+bp}{cls} \PY{o+ow}{in} \PY{n}{classes}\PY{p}{:}
                 \PY{n}{x\PYZus{}values} \PY{o}{=} \PY{n}{only\PYZus{}cls\PYZus{}feature\PYZus{}values}\PY{p}{(}\PY{n}{input\PYZus{}df}\PY{p}{,} \PY{n+nb+bp}{cls}\PY{p}{,} \PY{n}{x\PYZus{}feature}\PY{p}{)}
                 \PY{n}{y\PYZus{}values} \PY{o}{=} \PY{n}{only\PYZus{}cls\PYZus{}feature\PYZus{}values}\PY{p}{(}\PY{n}{input\PYZus{}df}\PY{p}{,} \PY{n+nb+bp}{cls}\PY{p}{,} \PY{n}{y\PYZus{}feature}\PY{p}{)}
                 \PY{n}{axis} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}values}\PY{p}{,} \PY{n}{y\PYZus{}values}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n}{handles}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{axis}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         
             \PY{k}{return} \PY{n}{handles}
         
         \PY{n}{scatter\PYZus{}plot\PYZus{}feature\PYZus{}pair}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{classes}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{petal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{classes}\PY{p}{)}\PY{p}{;}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{iris_lab_files/iris_lab_48_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{Scatterplot matrix}\label{scatterplot-matrix}

In addition to the distribution of individual features, it is also
useful to visualize the relationships between features. To do so, we can
create scatterplots for each pair of features, arranged in a 'matrix'
layout. Furthermore, by coloring the scatterplot according to 'class',
we can gain further insight into the nature of the correlations.

This section covers:

\begin{itemize}
\tightlist
\item
  Working with subplots
\end{itemize}

    \subsubsection{Cheating with pandas}\label{cheating-with-pandas}

If you are willing to accept their default, pandas already have
everything done for us (no need to use our previous functions).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}64}]:} \PY{k}{try}\PY{p}{:}
             \PY{k+kn}{from} \PY{n+nn}{pandas.plotting} \PY{k+kn}{import} \PY{n}{scatter\PYZus{}matrix}  \PY{c+c1}{\PYZsh{} BQNT has an old version of pandas, without plotting}
             \PY{n}{scatter\PYZus{}matrix}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{n}{diagonal}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{kde}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{n}{image} \PY{o}{=} \PY{n+nb+bp}{None}
         \PY{k}{except}\PY{p}{:}
             \PY{k+kn}{from} \PY{n+nn}{IPython.display} \PY{k+kn}{import} \PY{n}{Image}
             \PY{n}{image} \PY{o}{=} \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pandas\PYZus{}scatter\PYZus{}matrix.png}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{image}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{iris_lab_files/iris_lab_51_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    But this is not separated by class! No pretty colors!

The code below uses our functions to do a similar scatter matrix but
separated by classes.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}65}]:} \PY{k+kn}{from} \PY{n+nn}{itertools} \PY{k+kn}{import} \PY{n}{combinations}
         
         \PY{k}{def} \PY{n+nf}{scatterplot\PYZus{}matrix}\PY{p}{(}\PY{n}{input\PYZus{}df}\PY{p}{,} \PY{n}{classes}\PY{p}{,} \PY{n}{kde\PYZus{}in\PYZus{}diagonal}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} create the figure as a matrix of subplots, sharing x values in columns,}
             \PY{c+c1}{\PYZsh{} and y values in rows iff display\PYZus{}kde is False}
             \PY{n}{sharey} \PY{o}{=} \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sharey}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{row}\PY{l+s+s1}{\PYZsq{}}\PY{p}{\PYZcb{}} \PY{k}{if} \PY{o+ow}{not} \PY{n}{kde\PYZus{}in\PYZus{}diagonal} \PY{k}{else} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
             \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{axes\PYZus{}array} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{feature\PYZus{}names}\PY{p}{)}\PY{p}{,} \PY{n+nb}{len}\PY{p}{(}\PY{n}{feature\PYZus{}names}\PY{p}{)}\PY{p}{,} \PY{n}{sharex}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{col}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{sharey}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} loop over the diagonal}
             \PY{c+c1}{\PYZsh{} note: the enumerate(iterable) function returns tuples of (index, element) for all the elements in the iterable }
             \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{feature} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{feature\PYZus{}names}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} for the subplot at i,i: }
                 \PY{c+c1}{\PYZsh{} instead of plotting the feature against itself}
                 \PY{k}{if} \PY{n}{kde\PYZus{}in\PYZus{}diagonal}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} try implementing that function in the appendix}
                     \PY{n}{plot\PYZus{}kde}\PY{p}{(}\PY{n}{input\PYZus{}df}\PY{p}{,} \PY{n}{feature}\PY{p}{,} \PY{n}{classes}\PY{p}{,} \PY{n}{axes\PYZus{}array}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{n}{i}\PY{p}{]}\PY{p}{)}
                 \PY{k}{else}\PY{p}{:}
                     \PY{c+c1}{\PYZsh{} display name of the feature}
                     \PY{n}{plt}\PY{o}{.}\PY{n}{text}\PY{p}{(}
                         \PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{feature}\PY{p}{,}
                         \PY{n}{horizontalalignment}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{center}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                         \PY{n}{verticalalignment}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{center}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
                         \PY{n}{transform}\PY{o}{=}\PY{n}{axes\PYZus{}array}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{n}{i}\PY{p}{]}\PY{o}{.}\PY{n}{transAxes}
                     \PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} loop over the top right half of the matrix}
             \PY{c+c1}{\PYZsh{} note: combinations(iterable, 2) returns all pairs of features without considering order}
             \PY{c+c1}{\PYZsh{}       so in our case, we have [((0, \PYZsq{}sepal\PYZus{}length\PYZsq{}), (1, \PYZsq{}sepal\PYZus{}width\PYZsq{})), ...]}
             \PY{k}{for} \PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{feature1}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{n}{j}\PY{p}{,} \PY{n}{feature2}\PY{p}{)} \PY{o+ow}{in} \PY{n}{combinations}\PY{p}{(}\PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{feature\PYZus{}names}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
                 \PY{c+c1}{\PYZsh{} plot in the top right half (i, j)}
                 \PY{n}{scatter\PYZus{}plot\PYZus{}feature\PYZus{}pair}\PY{p}{(}
                     \PY{n}{input\PYZus{}df}\PY{p}{,}
                     \PY{n}{feature2}\PY{p}{,}
                     \PY{n}{feature1}\PY{p}{,}
                     \PY{n}{classes}\PY{p}{,}
                     \PY{n}{plt}\PY{o}{=}\PY{n}{axes\PYZus{}array}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{n}{j}\PY{p}{]}
                 \PY{p}{)}
         
                 \PY{c+c1}{\PYZsh{} plot the mirror image in the bottom left half (j, i)}
                 \PY{n}{handles} \PY{o}{=} \PY{n}{scatter\PYZus{}plot\PYZus{}feature\PYZus{}pair}\PY{p}{(}
                     \PY{n}{input\PYZus{}df}\PY{p}{,}
                     \PY{n}{feature1}\PY{p}{,}
                     \PY{n}{feature2}\PY{p}{,}
                     \PY{n}{classes}\PY{p}{,}
                     \PY{n}{plt}\PY{o}{=}\PY{n}{axes\PYZus{}array}\PY{p}{[}\PY{n}{j}\PY{p}{,}\PY{n}{i}\PY{p}{]}
                 \PY{p}{)}
         
             \PY{n}{plt}\PY{o}{.}\PY{n}{figlegend}\PY{p}{(}\PY{n}{handles}\PY{p}{,} \PY{n}{classes}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper center}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{scatterplot\PYZus{}matrix}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{n}{classes}\PY{p}{,} \PY{n}{kde\PYZus{}in\PYZus{}diagonal}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} you should also try this (see appendix 3 first):}
         \PY{c+c1}{\PYZsh{} scatterplot\PYZus{}matrix(iris\PYZus{}df, classes, kde\PYZus{}in\PYZus{}diagonal=True)}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{iris_lab_files/iris_lab_53_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We can easily see that setosa iris measurements form a separate cluster
for any two sets of features! This is not obvious from the kernel
densities for sepal measurements, though it can be seen in the bimodal
petal measurement kernel densities. The strongest correlation is between
petal width and petal length, as shown by the strong linearity in the
scatter plot, and a similar linear relationship is shared between all
classes of iris. This is not the case for e.g. petal length and sepal
length, which also exhibit fairly strong correlation within a class but
have different linear relationships between classes. These conclusions
can be used to interpret the results of clustering and regression
models.

    Fitting A Model

    Enough warming up, let's get to it!

To prep our data for building a model, we need to split it into three
main categories:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Training set
\item
  Validation set
\item
  Test set
\end{enumerate}

This is a critical best-practice in data science to guard against
overfitting!

Notice that below we use the \texttt{sklearn} function
\texttt{train\_test\_split} to partition the data randomly into the
three categories described above. The default train/test proportions are
75\%/25\%, but they can be manually configured as desired. Here we'll
use 80\%/20\%.

Additionally, if we had discovered that our data set contained
relatively few of one of the classes, we could specify
\texttt{stratify=y} to make sure that each split has equal
representation from all classes. (This could be important so we don't
end up with a split with 0 or very few examples from a particular class,
just by random chance.)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}66}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
         
         \PY{c+c1}{\PYZsh{}Renaming the DataFrame to stick with popular convention}
         \PY{n}{X} \PY{o}{=} \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Similar to Python dictionaries, pop() removes the column from the DataFrame and returns the corresponding Series}
         \PY{n}{y} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{pop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{X\PYZus{}train\PYZus{}all}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}all}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}val} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}all}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}all}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
\end{Verbatim}

    How would you get the counts of training examples across the Iris
classes?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} \PY{c+c1}{\PYZsh{} SOLUTION}
         \PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{to\PYZus{}frame}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{groupby}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to\PYZus{}frame}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{counts}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}67}]:}                  counts
         class                  
         Iris-setosa          33
         Iris-versicolor      33
         Iris-virginica       30
\end{Verbatim}
            
    Logistic Regression

Once we've loaded, cleaned, split, and visualized the data, we're ready
to try fitting some machine learning models! Logistic regression,
despite its name, can be viewed as a machine learning model for doing
classification. Logistic regression gives us prediction functions that
produce "soft classifications", namely probability distributions over
class labels. To get a hard classification, we will just choose the
class with the highest probability.

Scikit-Learn contains a LogisticRegression class, which can train a
model on your data and make predictions. There's a link to the full spec
below, but the most important methods are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  fit(X, y): Fits the model to the provided training data.
\item
  predict(X): Predicts the classification for test data.
\end{enumerate}

Full Spec:
http://scikit-learn.org/stable/modules/generated/sklearn.linear\_model.LogisticRegression.html

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}75}]:} \PY{c+c1}{\PYZsh{}Train a logistic regression model and use it to predict the species for the validation data}
         \PY{k+kn}{from} \PY{n+nn}{sklearn.linear\PYZus{}model} \PY{k+kn}{import} \PY{n}{LogisticRegression}
         \PY{n}{log\PYZus{}reg} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{p}{)}
         \PY{n}{log\PYZus{}reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{log\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}
         \PY{n}{y\PYZus{}pred}
         \PY{c+c1}{\PYZsh{} SOLUTION }
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}75}]:} array(['Iris-setosa', 'Iris-virginica', 'Iris-versicolor', 'Iris-setosa',
                'Iris-versicolor', 'Iris-versicolor', 'Iris-virginica',
                'Iris-versicolor', 'Iris-versicolor', 'Iris-setosa',
                'Iris-virginica', 'Iris-setosa', 'Iris-virginica',
                'Iris-virginica', 'Iris-virginica', 'Iris-virginica',
                'Iris-virginica', 'Iris-setosa', 'Iris-setosa', 'Iris-virginica',
                'Iris-setosa', 'Iris-virginica', 'Iris-versicolor',
                'Iris-virginica'], dtype=object)
\end{Verbatim}
            
    For any model, we need a way of assessing how good our predictions are.
LogisticRegression has a method, score(X, y), which returns the mean
accuracy, or the percent of samples correctly classified, on the given
test data and provided labels. It first uses the trained model to
predict the classes for the samples in X, then compares those to the
true classes in y.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}78}]:} \PY{c+c1}{\PYZsh{}Output the score of your model on your validation set}
         \PY{n}{log\PYZus{}reg}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{,} \PY{n}{y\PYZus{}val}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} SOLUTION}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}78}]:} 1.0
\end{Verbatim}
            
    Confusion Matrix

Logistic regression appears to do a pretty good job of classifying the
samples by species. We may want to look more carefully at where the
classifier succeeded and where it failed. Doing so can illuminate
similarities between classes or suggest the need for additional features
to help the model differentiate between classes.

One tool for understanding the success of a classifier is a confusion
matrix, which breaks down the predicted classifications by true class:
for each true class, it shows how many samples were predicted to be in
each possible class. Pandas contains a tool, crosstab, for creating
them.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}79}]:} \PY{c+c1}{\PYZsh{} check if this logistic regression model has been fit}
         \PY{k}{if} \PY{n+nb}{hasattr}\PY{p}{(}\PY{n}{log\PYZus{}reg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{coef\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o+ow}{and} \PY{n}{log\PYZus{}reg}\PY{o}{.}\PY{n}{coef\PYZus{}} \PY{o+ow}{is} \PY{o+ow}{not} \PY{n+nb+bp}{None}\PY{p}{:}
             \PY{n}{predictions} \PY{o}{=} \PY{n}{log\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}
             \PY{k}{print}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{crosstab}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{predictions}\PY{p}{,} \PY{n}{rownames} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{colnames}\PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Predicted Class  Iris-setosa  Iris-versicolor  Iris-virginica
True Class                                                   
Iris-setosa                7                0               0
Iris-versicolor            0                6               0
Iris-virginica             0                0              11

    \end{Verbatim}

    Hyperparameters

Recall that hyperparameters are parameters of the machine learning model
itself that can be adjusted to produce a better fitting model. One of
the most important applications of hyperparameters is in regularization
, a technique to help avoid overfitting. Regularization essentially
sacrifices the "best" model for the training data in return for a model
that can generalize better to test data.

One form of regularization works by penalizing models that have
coefficients that are "too big". For example, in a linear model
\(y = a_1 x + a_2\), we might want to avoid models that have very steep
slopes or very large intercepts. Controlling \(a_1\) and \(a_2\) allows
us to do so.

Two forms of regularization are commonly used for Logistic Regression:
l1 and l2 (pronounced "ell 1" and "ell 2") 1. In l2 regularization, we
control the size of the coefficients by penalizing the model for having
a large sum of squared coefficients (eg, \(a_1^2 + a_2^2\)). This is the
default regularization in Scikit-Learn's LogisticRegression.\\
3. In l1 regularization, we control the size of the coefficients by
penalizing the model for having a large sum of the absolute value of the
coefficients (eg, \(|a_1| + |a_2|\)). l1 regularization favors "sparse
solutions". A sparse solution is when one or more of the feature weights
is 0. The more l1 regularization we have, the sparser the solution we
get. For this reason, l1 regularization can be used for feature
selection.

To try different types of regularization in Scikit-Learn, you need to
initialize LogisticRegression with two parameters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  penalty: Type of regularization; a string, either 'l1' or 'l2'
\item
  C: The inverse of regularization strength; a positive float that
  determines how much to penalize the model for large coefficients. Note
  that since it's the inverse, smaller values of C mean a more
  regularized model (i.e. smaller coefficients).
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}89}]:} \PY{c+c1}{\PYZsh{} Try some different combinations of C and penalty and see how they affect the score on the validation set. }
         \PY{n}{log\PYZus{}reg} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{C}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{n}{penalty}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{log\PYZus{}reg}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
         \PY{k}{if} \PY{n+nb}{hasattr}\PY{p}{(}\PY{n}{log\PYZus{}reg}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{coef\PYZus{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{o+ow}{and} \PY{n}{log\PYZus{}reg}\PY{o}{.}\PY{n}{coef\PYZus{}} \PY{o+ow}{is} \PY{o+ow}{not} \PY{n+nb+bp}{None}\PY{p}{:}
             \PY{n}{predictions} \PY{o}{=} \PY{n}{log\PYZus{}reg}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}
             \PY{k}{print}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{crosstab}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{predictions}\PY{p}{,} \PY{n}{rownames} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{colnames}\PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{p}{)}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} SOLUTION}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Predicted Class  Iris-setosa  Iris-versicolor  Iris-virginica
True Class                                                   
Iris-setosa                7                0               0
Iris-versicolor            0                4               2
Iris-virginica             0                0              11

    \end{Verbatim}

    Using GridSearch for hyperparameter tuning

Scikit-Learn has a tool to make this kind of hyperparameter tuning
easier.

Recall that cross validiation is another technique for assessing the
quality of a model. Instead of splitting the training data once into a
test set and a validation set, in cross validation, the data is divided
into \(n\) subsets, called folds, and the model is trained \(n\) times,
each time witholding a different fold to use as the validation set.

Cross validation can be helpful for small datasets, since every data
point is used to assess the model performance, rather than just a fixed
subset.

In Scikit-Learn, GridSearchCV is used to accomplish this kind of
parameter tuning. Given a Scikit-Learn estimator object (eg,
LogisticRegression), a dictionary listing all the parameters to try, and
a scoring method, GridSearchCV will train a model for every parameter
combination and show the score for every combination.

Once GridSearchCV has fit the model, you can access cv\_results\_, a
matrix of results, as well as best\_estimator\_, best\_score\_, and
others. See
http://scikit-learn.org/stable/modules/generated/sklearn.model\_selection.GridSearchCV.html
for full details.

Since we are working on a classification problem, we will use accuracy
as our scoring method, but GridSearchCV (and Scikit-Learn) supports many
other types of scores, assuming they're supported by the estimator
object. See
http://scikit-learn.org/stable/modules/model\_evaluation.html\#scoring-parameter
for a complete list.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}90}]:} \PY{c+c1}{\PYZsh{}Using GridSearchCV, train models on for a wide variety of hyperparameters}
         
         \PY{k+kn}{from} \PY{n+nn}{sklearn.model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{GridSearchCV}
         \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{[}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+m+mf}{1e\PYZhy{}5}\PY{p}{,} \PY{l+m+mf}{0.0001}\PY{p}{,} \PY{l+m+mf}{0.001}\PY{p}{,} \PY{l+m+mf}{0.01}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{10.0}\PY{p}{,} \PY{l+m+mf}{100.0}\PY{p}{,} \PY{l+m+mf}{1000.0}\PY{p}{,} \PY{l+m+mf}{10000.0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{penalty}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{]}
         \PY{n}{log\PYZus{}reg} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{p}{)}
         \PY{n}{grid\PYZus{}search} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{log\PYZus{}reg}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{return\PYZus{}train\PYZus{}score}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
         \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}all}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}all}\PY{p}{)}
         \PY{n}{results} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}There\PYZsq{}s lots of data in the results dataframe \PYZhy{} we\PYZsq{}ll just look at a few columns here, but feel free to explore}
         \PY{n}{results}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{param\PYZus{}C}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{param\PYZus{}penalty}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rank\PYZus{}test\PYZus{}score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}90}]:}    param\_C param\_penalty  mean\_test\_score  rank\_test\_score
         0    1e-05            l1         0.333333               17
         1    1e-05            l2         0.341667               14
         2   0.0001            l1         0.333333               17
         3   0.0001            l2         0.341667               14
         4    0.001            l1         0.333333               17
         5    0.001            l2         0.341667               14
         6     0.01            l1         0.325000               20
         7     0.01            l2         0.675000               13
         8      0.1            l1         0.700000               12
         9      0.1            l2         0.816667               11
         10       1            l1         0.941667                9
         11       1            l2         0.941667                9
         12      10            l1         0.958333                7
         13      10            l2         0.966667                4
         14     100            l1         0.958333                7
         15     100            l2         0.975000                1
         16    1000            l1         0.966667                4
         17    1000            l2         0.975000                1
         18   10000            l1         0.966667                4
         19   10000            l2         0.975000                1
\end{Verbatim}
            
    Trying other models

Logistic Regression is a very common basic classification algorithm, but
there are others you can also try.

K Nearest Neighbors

In K Nearest Neighbors classification, the prediction function
identifies the K "nearest neighbors" to an input feature vector, and
predicts the most frequently occurring class among those neighbors. The
nearest neighbors are typically found using the standard euclidean
distance, though any other similarity score may be used instead.The K in
K nearest neighbors is a hyperparameter: small values of K will provide
a more flexible, granular fit, but will be sensitive to noise. Larger
values of K will be more resilient to noise but less likely to pick up
small variations in the boundaries.

To implement a K Nearest Neighbors model, use Scikit-Learn's
KNeighborsClassifier. Initialize it with n\_neighbors and then fit,
predict, and score your model just like LogisticRegression. For more
details, see
http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}113}]:} \PY{c+c1}{\PYZsh{} Try out K Nearest Neighbors}
          \PY{k+kn}{from} \PY{n+nn}{sklearn.neighbors} \PY{k+kn}{import} \PY{n}{KNeighborsClassifier} 
          \PY{c+c1}{\PYZsh{}Try out KNN with hp tuning}
          \PY{k+kn}{from} \PY{n+nn}{sklearn.model\PYZus{}selection} \PY{k+kn}{import} \PY{n}{GridSearchCV}
          \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{[}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}neighbors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{\PYZcb{}}\PY{p}{]}
          \PY{n}{KNN\PYZus{}model} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{p}{)}
          \PY{n}{grid\PYZus{}search} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{KNN\PYZus{}model}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{return\PYZus{}train\PYZus{}score}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
          \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}all}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}all}\PY{p}{)}
          \PY{n}{results} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{)}
          \PY{c+c1}{\PYZsh{}There\PYZsq{}s lots of data in the results dataframe \PYZhy{} we\PYZsq{}ll just look at a few columns here, but feel free to explore}
          \PY{n}{results}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{param\PYZus{}n\PYZus{}neighbors}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rank\PYZus{}test\PYZus{}score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}
          
          \PY{n}{KNN\PYZus{}model\PYZus{}opt} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors} \PY{o}{=} \PY{l+m+mi}{3}\PY{p}{)}
          \PY{n}{KNN\PYZus{}model\PYZus{}opt}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{predictions} \PY{o}{=} \PY{n}{KNN\PYZus{}model\PYZus{}opt}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}
          \PY{k}{print}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{crosstab}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{predictions}\PY{p}{,} \PY{n}{rownames} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{colnames}\PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{p}{)}\PY{p}{)}
          \PY{c+c1}{\PYZsh{} SOLUTION}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Predicted Class  Iris-setosa  Iris-versicolor  Iris-virginica
True Class                                                   
Iris-setosa                7                0               0
Iris-versicolor            0                6               0
Iris-virginica             0                0              11

    \end{Verbatim}

    Random Forest Classifier

Random forest is an aggregation of another type of model, decision
trees. A decision tree uses a series of decisions to try to classify a
sample. For example, possible decision tree logic could be:

If sepal length is less than 4.5, classify as versicolor. If sepal
length is greater than 4.5, then look at petal length; if petal length
is greater than 1.5, classify as setosa, otherwise classify as
virginica.

In Random Forest classification, many decision trees are built on the
data, and each will be different because of some randomness introduced
in the tree-building process. These trees then all "vote" on the
classification of an input. One benefit of random forest models is
insight into feature selection, since the multiple trees can generate
data about which features are most relevant to each decision.

In Scikit-Learn, use RandomForestClassifier to implement a Random Forest
model. Initialize it with n\_estimators, the number of trees in the
forest. It is also helpful to initialize the random\_state so you can
reproduce your model. Fit, predict, and score work as they do for the
other estimators, but you can also take a look at feature\_importances\_
to see which features are most important in your model. See the docs at
http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}114}]:} \PY{c+c1}{\PYZsh{} Try out Random Forest Classifier}
          \PY{k+kn}{from} \PY{n+nn}{sklearn.ensemble} \PY{k+kn}{import} \PY{n}{RandomForestClassifier}
          \PY{n}{random\PYZus{}forest} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{,} \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
          \PY{n}{random\PYZus{}forest}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{predictions} \PY{o}{=} \PY{n}{random\PYZus{}forest}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}
          \PY{k}{print}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{crosstab}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{predictions}\PY{p}{,} \PY{n}{rownames} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{colnames}\PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{p}{)}\PY{p}{)}
          \PY{c+c1}{\PYZsh{} SOLUTION}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Predicted Class  Iris-setosa  Iris-versicolor  Iris-virginica
True Class                                                   
Iris-setosa                7                0               0
Iris-versicolor            0                5               1
Iris-virginica             0                1              10

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/weight\_boosting.py:29: DeprecationWarning: numpy.core.umath\_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.
  from numpy.core.umath\_tests import inner1d

    \end{Verbatim}

    Comparing Between Models

With three different models, all with pretty good performance, we need
some way to choose the best. Because the dataset is so small, looking at
the score on the validation set is very sensitive to variations in how
the data was split. Cross validation will be helpful here; training and
scoring the different models on different folds of the data will give us
a better sense of overall performance.

GridSearchCV is written for only one model. We can get around that by
writing a wrapper class that takes the model we want to use as a
hyperparameter. GridSearchCV will be able to use our wrapper as long as
we provide the standard API.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}115}]:} \PY{k+kn}{from} \PY{n+nn}{sklearn.base} \PY{k+kn}{import} \PY{n}{BaseEstimator}\PY{p}{,} \PY{n}{RegressorMixin}\PY{p}{,} \PY{n}{ClassifierMixin}
          
          \PY{k}{class} \PY{n+nc}{CrossValidationModelWrapper}\PY{p}{(}\PY{n}{BaseEstimator}\PY{p}{,} \PY{n}{RegressorMixin}\PY{p}{)}\PY{p}{:}  
              \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}sklearn wrapper for our classifiers\PYZdq{}\PYZdq{}\PYZdq{}}
               
              \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{model}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LogisticRegression}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{n\PYZus{}neighbors}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{C}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{penalty}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{:}        
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{model} \PY{o}{=} \PY{n}{model}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}neighbors} \PY{o}{=} \PY{n}{n\PYZus{}neighbors}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}estimators} \PY{o}{=} \PY{n}{n\PYZus{}estimators}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{C} \PY{o}{=} \PY{n}{C}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{penalty} \PY{o}{=} \PY{n}{penalty} 
          
              \PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n+nb+bp}{None}\PY{p}{)}\PY{p}{:}
                  \PY{k}{if} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{model} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{LogisticRegression}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
                      \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{classifier} \PY{o}{=} \PY{n}{LogisticRegression}\PY{p}{(}\PY{n}{C}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{C}\PY{p}{,} \PY{n}{penalty}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{penalty}\PY{p}{)}
                  \PY{k}{elif} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{model} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{KNeighborsClassifier}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
                      \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{classifier} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}neighbors}\PY{p}{)}
                  \PY{k}{elif} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{model} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RandomForestClassifier}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{p}{:}
                      \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{classifier} \PY{o}{=} \PY{n}{RandomForestClassifier}\PY{p}{(}\PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{,} \PY{n}{n\PYZus{}estimators}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{n\PYZus{}estimators}\PY{p}{)}
                  \PY{k}{else}\PY{p}{:}
                      \PY{k}{raise} \PY{n+ne}{ValueError}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Unrecognized Classifier}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                  
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{classifier}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
          
                  \PY{k}{return} \PY{n+nb+bp}{self}
          
              \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n+nb+bp}{None}\PY{p}{)}\PY{p}{:}
                  \PY{k}{try}\PY{p}{:}
                      \PY{n+nb}{getattr}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{classifier}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
                  \PY{k}{except} \PY{n+ne}{AttributeError}\PY{p}{:}
                      \PY{k}{raise} \PY{n+ne}{RuntimeError}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{You must train classifer before predicting data!}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
          
                  \PY{k}{return}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{classifier}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}
          
              \PY{k}{def} \PY{n+nf}{score}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{n+nb+bp}{None}\PY{p}{)}\PY{p}{:}
                  \PY{k}{return}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{classifier}\PY{o}{.}\PY{n}{score}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{)} 
              
\end{Verbatim}

    We can now use this wrapper to perform GridSearch. Customize the
parameters in the parameter grid below to find the optimal sets of
hyperparameters.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}120}]:} \PY{n}{param\PYZus{}grid} \PY{o}{=} \PY{p}{[}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LogisticRegression}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{,} \PY{l+m+mf}{10.0}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{penalty}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{l2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{p}{\PYZcb{}}\PY{p}{,} 
                        \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{KNeighborsClassifier}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}neighbors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{,}
                        \PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{RandomForestClassifier}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{n\PYZus{}estimators}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{p}{[}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{15}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{25}\PY{p}{]}\PY{p}{\PYZcb{}}\PY{p}{]}
          \PY{n}{cv\PYZus{}wrapper} \PY{o}{=} \PY{n}{CrossValidationModelWrapper}\PY{p}{(}\PY{p}{)}
          \PY{n}{grid\PYZus{}search} \PY{o}{=} \PY{n}{GridSearchCV}\PY{p}{(}\PY{n}{cv\PYZus{}wrapper}\PY{p}{,} \PY{n}{param\PYZus{}grid}\PY{p}{,} \PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{,} \PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{accuracy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{return\PYZus{}train\PYZus{}score}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}
          \PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train\PYZus{}all}\PY{p}{,} \PY{n}{y\PYZus{}train\PYZus{}all}\PY{p}{)}
          \PY{n}{results} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{grid\PYZus{}search}\PY{o}{.}\PY{n}{cv\PYZus{}results\PYZus{}}\PY{p}{)}
          \PY{n}{results}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{param\PYZus{}model}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{param\PYZus{}n\PYZus{}neighbors}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{param\PYZus{}n\PYZus{}estimators}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} 
                   \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{param\PYZus{}C}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{param\PYZus{}penalty}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{mean\PYZus{}test\PYZus{}score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rank\PYZus{}test\PYZus{}score}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{fillna}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}120}]:}                param\_model param\_n\_neighbors param\_n\_estimators param\_C  \textbackslash{}
          0       LogisticRegression                 -                  -     0.1   
          1       LogisticRegression                 -                  -     0.1   
          2       LogisticRegression                 -                  -       1   
          3       LogisticRegression                 -                  -       1   
          4       LogisticRegression                 -                  -      10   
          5       LogisticRegression                 -                  -      10   
          6     KNeighborsClassifier                 1                  -       -   
          7     KNeighborsClassifier                 3                  -       -   
          8     KNeighborsClassifier                 5                  -       -   
          9     KNeighborsClassifier                 6                  -       -   
          10    KNeighborsClassifier                 8                  -       -   
          11    KNeighborsClassifier                10                  -       -   
          12  RandomForestClassifier                 -                  5       -   
          13  RandomForestClassifier                 -                 10       -   
          14  RandomForestClassifier                 -                 15       -   
          15  RandomForestClassifier                 -                 20       -   
          16  RandomForestClassifier                 -                 25       -   
          
             param\_penalty  mean\_test\_score  rank\_test\_score  
          0             l1         0.691667               17  
          1             l2         0.791667               16  
          2             l1         0.950000               14  
          3             l2         0.950000               14  
          4             l1         0.958333               10  
          5             l2         0.975000                4  
          6              -         0.975000                4  
          7              -         0.983333                1  
          8              -         0.983333                1  
          9              -         0.983333                1  
          10             -         0.958333               10  
          11             -         0.975000                4  
          12             -         0.966667                7  
          13             -         0.958333               10  
          14             -         0.958333               10  
          15             -         0.966667                7  
          16             -         0.966667                7  
\end{Verbatim}
            
    Train and test your final model!

Choose your best performing model from above and use it to predict the
species of your test data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}121}]:} \PY{c+c1}{\PYZsh{} SOLUTION}
          \PY{n}{KNN\PYZus{}model\PYZus{}opt} \PY{o}{=} \PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors} \PY{o}{=} \PY{l+m+mi}{3}\PY{p}{)}
          \PY{n}{KNN\PYZus{}model\PYZus{}opt}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n}{predictions} \PY{o}{=} \PY{n}{KNN\PYZus{}model\PYZus{}opt}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}val}\PY{p}{)}
          \PY{k}{print}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{crosstab}\PY{p}{(}\PY{n}{y\PYZus{}val}\PY{p}{,} \PY{n}{predictions}\PY{p}{,} \PY{n}{rownames} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{True Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{colnames}\PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Predicted Class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Predicted Class  Iris-setosa  Iris-versicolor  Iris-virginica
True Class                                                   
Iris-setosa                7                0               0
Iris-versicolor            0                6               0
Iris-virginica             0                0              11

    \end{Verbatim}

    \subsection{Appendix 1: Numpy}\label{appendix-1-numpy}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}122}]:} \PY{k}{def} \PY{n+nf}{load\PYZus{}csv}\PY{p}{(}\PY{n}{path}\PY{p}{,} \PY{n}{loader}\PY{o}{=}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n+nb}{open}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}\PY{p}{:}
              \PY{k}{return} \PY{n}{loader}\PY{p}{(}\PY{n}{path}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}
          
          \PY{n}{iris\PYZus{}text} \PY{o}{=} \PY{n}{load\PYZus{}csv}\PY{p}{(}\PY{n}{iris\PYZus{}csv\PYZus{}path}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}123}]:} \PY{n}{iris\PYZus{}text}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{200}\PY{p}{]}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}123}]:} 'sepal\_length,sepal\_width,petal\_length,petal\_width,class\textbackslash{}n5.1,3.5,1.4,0.2,Iris-setosa\textbackslash{}n4.9,3.0,1.4,0.2,Iris-setosa\textbackslash{}n4.7,3.2,1.3,0.2,Iris-setosa\textbackslash{}n4.6,3.1,1.5,0.2,Iris-setosa\textbackslash{}n5.0,3.6,1.4,0.2,Iris-setosa\textbackslash{}n5.4,'
\end{Verbatim}
            
    Obviously, this format is not going to be very easy to work with!

Our next thought might be to parse the csv into a list of lists, but
using the popular library \texttt{numpy}, we can do even better. The
following code loads the csv into an n-dimensional array, called an
\texttt{ndarray} in numpy parlance.

This data structure is similar in principal to python's native lists,
but are faster, more memory efficient, and have some handy additional
features.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}124}]:} \PY{k}{def} \PY{n+nf}{array\PYZus{}loader}\PY{p}{(}\PY{n}{path}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}\PY{p}{:}
              \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{genfromtxt}\PY{p}{(}\PY{n}{path}\PY{p}{,} \PY{n}{delimiter}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{autostrip}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{o}{*}\PY{o}{*}\PY{n}{kwargs}\PY{p}{)}
          
          \PY{n}{iris\PYZus{}ndarray} \PY{o}{=} \PY{n}{load\PYZus{}csv}\PY{p}{(}\PY{n}{iris\PYZus{}csv\PYZus{}path}\PY{p}{,} \PY{n}{loader}\PY{o}{=}\PY{n}{array\PYZus{}loader}\PY{p}{,} \PY{n}{names}\PY{o}{=}\PY{n+nb+bp}{True}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n+nb+bp}{None}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
/usr/local/lib/python2.7/dist-packages/ipykernel\_launcher.py:2: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.
  

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}125}]:} \PY{n}{iris\PYZus{}ndarray}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}125}]:} array([(5.1, 3.5, 1.4, 0.2, 'Iris-setosa'),
                 (4.9, 3. , 1.4, 0.2, 'Iris-setosa'),
                 (4.7, 3.2, 1.3, 0.2, 'Iris-setosa'),
                 (4.6, 3.1, 1.5, 0.2, 'Iris-setosa'),
                 (5. , 3.6, 1.4, 0.2, 'Iris-setosa'),
                 (5.4, 3.9, 1.7, 0.4, 'Iris-setosa'),
                 (4.6, 3.4, 1.4, 0.3, 'Iris-setosa'),
                 (5. , 3.4, 1.5, 0.2, 'Iris-setosa'),
                 (4.4, 2.9, 1.4, 0.2, 'Iris-setosa'),
                 (4.9, 3.1, 1.5, 0.1, 'Iris-setosa')],
                dtype=[('sepal\_length', '<f8'), ('sepal\_width', '<f8'), ('petal\_length', '<f8'), ('petal\_width', '<f8'), ('class', 'S15')])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}126}]:} \PY{n}{iris\PYZus{}ndarray}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}126}]:} (150,)
\end{Verbatim}
            
    An important point to note is that the \texttt{ndarray} is a homogenous
data-structure. Since our CSV has both floats and strings,
\texttt{numpy} gives us a one-dimensional array where each row is a
tuple!

One solution is to encode the the classes as floats:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}127}]:} \PY{k}{def} \PY{n+nf}{class\PYZus{}converter}\PY{p}{(}\PY{n}{flower\PYZus{}class}\PY{p}{)}\PY{p}{:}
              \PY{n}{classes} \PY{o}{=} \PY{p}{[}\PY{l+s+sa}{b}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iris\PYZhy{}setosa}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{b}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iris\PYZhy{}versicolor}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+sa}{b}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iris\PYZhy{}virginica}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
              \PY{k}{return} \PY{n+nb}{float}\PY{p}{(}\PY{n}{classes}\PY{o}{.}\PY{n}{index}\PY{p}{(}\PY{n}{flower\PYZus{}class}\PY{p}{)}\PY{p}{)}
          
          \PY{n}{iris\PYZus{}ndarray} \PY{o}{=} \PY{n}{load\PYZus{}csv}\PY{p}{(}\PY{n}{iris\PYZus{}csv\PYZus{}path}\PY{p}{,} \PY{n}{loader}\PY{o}{=}\PY{n}{array\PYZus{}loader}\PY{p}{,} \PY{n}{skip\PYZus{}header}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{converters}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+m+mi}{4}\PY{p}{:} \PY{n}{class\PYZus{}converter}\PY{p}{\PYZcb{}}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}128}]:} \PY{n}{iris\PYZus{}ndarray}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}128}]:} array([[5.1, 3.5, 1.4, 0.2, 0. ],
                 [4.9, 3. , 1.4, 0.2, 0. ],
                 [4.7, 3.2, 1.3, 0.2, 0. ],
                 [4.6, 3.1, 1.5, 0.2, 0. ],
                 [5. , 3.6, 1.4, 0.2, 0. ],
                 [5.4, 3.9, 1.7, 0.4, 0. ],
                 [4.6, 3.4, 1.4, 0.3, 0. ],
                 [5. , 3.4, 1.5, 0.2, 0. ],
                 [4.4, 2.9, 1.4, 0.2, 0. ],
                 [4.9, 3.1, 1.5, 0.1, 0. ]])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}129}]:} \PY{n}{iris\PYZus{}ndarray}\PY{o}{.}\PY{n}{shape}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}129}]:} (150, 5)
\end{Verbatim}
            
    When doing matrix operations, it is often to transpose some of the
inputs. \texttt{numpy} makes this easy (compare these values to those
from the first column above):

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}130}]:} \PY{n}{iris\PYZus{}ndarray}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}130}]:} array([5.1, 4.9, 4.7, 4.6, 5. , 5.4, 4.6, 5. , 4.4, 4.9, 5.4, 4.8, 4.8,
                 4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5.1, 5.4, 5.1, 4.6, 5.1, 4.8, 5. ,
                 5. , 5.2, 5.2, 4.7, 4.8, 5.4, 5.2, 5.5, 4.9, 5. , 5.5, 4.9, 4.4,
                 5.1, 5. , 4.5, 4.4, 5. , 5.1, 4.8, 5.1, 4.6, 5.3, 5. , 7. , 6.4,
                 6.9, 5.5, 6.5, 5.7, 6.3, 4.9, 6.6, 5.2, 5. , 5.9, 6. , 6.1, 5.6,
                 6.7, 5.6, 5.8, 6.2, 5.6, 5.9, 6.1, 6.3, 6.1, 6.4, 6.6, 6.8, 6.7,
                 6. , 5.7, 5.5, 5.5, 5.8, 6. , 5.4, 6. , 6.7, 6.3, 5.6, 5.5, 5.5,
                 6.1, 5.8, 5. , 5.6, 5.7, 5.7, 6.2, 5.1, 5.7, 6.3, 5.8, 7.1, 6.3,
                 6.5, 7.6, 4.9, 7.3, 6.7, 7.2, 6.5, 6.4, 6.8, 5.7, 5.8, 6.4, 6.5,
                 7.7, 7.7, 6. , 6.9, 5.6, 7.7, 6.3, 6.7, 7.2, 6.2, 6.1, 6.4, 7.2,
                 7.4, 7.9, 6.4, 6.3, 6.1, 7.7, 6.3, 6.4, 6. , 6.9, 6.7, 6.9, 5.8,
                 6.8, 6.7, 6.7, 6.3, 6.5, 6.2, 5.9])
\end{Verbatim}
            
    \subsection{Appendix 2: Pandas}\label{appendix-2-pandas}

    Here's an example of leveraging data labels in order to merge two
\texttt{Series} into a \texttt{DataFrame}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}131}]:} \PY{n}{ibm\PYZus{}eod\PYZus{}prices} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{159.55}\PY{p}{,} \PY{l+m+mf}{155.88}\PY{p}{,} \PY{l+m+mf}{153.50}\PY{p}{,} \PY{l+m+mf}{153.60}\PY{p}{,} \PY{l+m+mf}{154.36}\PY{p}{,} \PY{l+m+mf}{154.06}\PY{p}{]}
          \PY{n}{ibm\PYZus{}dates} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DatetimeIndex}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2017\PYZhy{}10\PYZhy{}23}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2017\PYZhy{}10\PYZhy{}24}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2017\PYZhy{}10\PYZhy{}25}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2017\PYZhy{}10\PYZhy{}26}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2017\PYZhy{}10\PYZhy{}30}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2017\PYZhy{}10\PYZhy{}31}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
          \PY{n}{ibm\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{ibm\PYZus{}eod\PYZus{}prices}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{ibm\PYZus{}dates}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}132}]:} \PY{c+c1}{\PYZsh{} mix up the dates a little}
          \PY{n}{aapl\PYZus{}eod\PYZus{}prices} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{156.17}\PY{p}{,} \PY{l+m+mf}{157.10}\PY{p}{,} \PY{l+m+mf}{156.41}\PY{p}{,} \PY{l+m+mf}{163.05}\PY{p}{,} \PY{l+m+mf}{166.72}\PY{p}{,} \PY{l+m+mf}{169.04}\PY{p}{]}
          \PY{n}{aapl\PYZus{}dates} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DatetimeIndex}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2017\PYZhy{}10\PYZhy{}23}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2017\PYZhy{}10\PYZhy{}24}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2017\PYZhy{}10\PYZhy{}25}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2017\PYZhy{}10\PYZhy{}27}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2017\PYZhy{}10\PYZhy{}30}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2017\PYZhy{}10\PYZhy{}31}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
          \PY{n}{aapl\PYZus{}series} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{Series}\PY{p}{(}\PY{n}{aapl\PYZus{}eod\PYZus{}prices}\PY{p}{,} \PY{n}{index}\PY{o}{=}\PY{n}{aapl\PYZus{}dates}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}133}]:} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{IBM}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{ibm\PYZus{}series}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{AAPL}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{n}{aapl\PYZus{}series}\PY{p}{\PYZcb{}}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}133}]:}               AAPL     IBM
          2017-10-23  156.17  159.55
          2017-10-24  157.10  155.88
          2017-10-25  156.41  153.50
          2017-10-26     NaN  153.60
          2017-10-27  163.05     NaN
          2017-10-30  166.72  154.36
          2017-10-31  169.04  154.06
\end{Verbatim}
            
    Often the dataframe requires some preprocessing before it's suitable in
a model

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}134}]:} \PY{n}{iris\PYZus{}features\PYZus{}copy} \PY{o}{=} \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{class}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}135}]:} \PY{n}{iris\PYZus{}features\PYZus{}copy}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}interaction}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{iris\PYZus{}features\PYZus{}copy}\PY{o}{.} \PYZbs{}
              \PY{n+nb}{apply}\PY{p}{(}\PY{k}{lambda} \PY{n}{sample}\PY{p}{:} \PY{n}{sample}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{*} \PY{n}{sample}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}width}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}136}]:} \PY{n}{iris\PYZus{}features\PYZus{}copy}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}136}]:}    sepal\_length  sepal\_width  petal\_length  petal\_width  sepal\_interaction
          0           5.1          3.5           1.4          0.2              17.85
          1           4.9          3.0           1.4          0.2              14.70
          2           4.7          3.2           1.3          0.2              15.04
          3           4.6          3.1           1.5          0.2              14.26
          4           5.0          3.6           1.4          0.2              18.00
\end{Verbatim}
            
    \subsection{Appendix 3: Kernel density
plot}\label{appendix-3-kernel-density-plot}

The histogram produced in the previous section was a good start.
However, the shape of the histogram can be sensitive to the number of
bins, so it is hard to say if this distribution is really multi-modal
(i.e. multiple peaks), or if we have simply chosen an inconvenient bin
size. Furthermore, for some features (like sepal length and width) the
histograms overlap, so it's hard to read. An alternative way to
visualize the distribution of a feature is to use a kernel density
estimate, which estimates the probability density function of the
feature values (using a Gaussian kernel in this case). This section
covers:

\begin{itemize}
\tightlist
\item
  Creating a kernel density estimate of the distribution of a feature
\item
  Creating a line plot
\item
  Interpreting the plot
\end{itemize}

    \subsubsection{Line plots}\label{line-plots}

Simply pass an array of \(x\) values and its corresponding \(y\) values
to \texttt{plt.plot} to draw a line plot.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}137}]:} \PY{n}{x\PYZus{}values} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{num}\PY{o}{=}\PY{l+m+mi}{11}\PY{p}{)}  \PY{c+c1}{\PYZsh{} array of 11 numbers, from \PYZhy{}5 to 5, evenly spaced}
          \PY{n}{x\PYZus{}values}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}137}]:} array([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}138}]:} \PY{k}{def} \PY{n+nf}{square}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
              \PY{k}{return} \PY{n}{x} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}
          
          \PY{n}{y\PYZus{}values} \PY{o}{=} \PY{n}{square}\PY{p}{(}\PY{n}{x\PYZus{}values}\PY{p}{)}
          \PY{n}{y\PYZus{}values}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}138}]:} array([25., 16.,  9.,  4.,  1.,  0.,  1.,  4.,  9., 16., 25.])
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}139}]:} \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}values}\PY{p}{,} \PY{n}{y\PYZus{}values}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}x\PYZca{}2\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{;}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{iris_lab_files/iris_lab_106_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsubsection{Kernel density}\label{kernel-density}

In order to plot a kernel density estimate instead of a histogram, we
need to use \texttt{scipy.stats.gaussian\_kde}.

This function needs:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the one-dimensional data whose density we want to estimate
\item
  the kernel width of the gaussian to apply
\end{enumerate}

It returns a function that returns the estimated probability density for
a given value.

    \subsubsection{Exercise: Kernel density estimates separated by
classes}\label{exercise-kernel-density-estimates-separated-by-classes}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Replace the \texttt{...} with code to plot the KDE (kernel density
  estimate) of the given \texttt{feature\_name}.
\item
  Create a new cell to see what the density is for each feature (looping
  over \texttt{feature\_names}):
\end{enumerate}

which feature seems to be the best for discriminating species? Which is
the worst?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}155}]:} \PY{k+kn}{import} \PY{n+nn}{scipy.stats} \PY{k+kn}{as} \PY{n+nn}{stats}
          
          \PY{k}{def} \PY{n+nf}{plot\PYZus{}kde}\PY{p}{(}\PY{n}{input\PYZus{}df}\PY{p}{,} \PY{n}{feature\PYZus{}name}\PY{p}{,} \PY{n}{classes}\PY{p}{,} \PY{n}{plt}\PY{o}{=}\PY{n}{plt}\PY{p}{)}\PY{p}{:}
              \PY{c+c1}{\PYZsh{} prepare the kernel density estimate parameters}
              \PY{n}{min\PYZus{}val} \PY{o}{=} \PY{n}{input\PYZus{}df}\PY{p}{[}\PY{n}{feature\PYZus{}name}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}
              \PY{n}{max\PYZus{}val} \PY{o}{=} \PY{n}{input\PYZus{}df}\PY{p}{[}\PY{n}{feature\PYZus{}name}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}
              \PY{c+c1}{\PYZsh{} perform a rough estimate for an appropriate kernel width (alternatively, accept the default)}
              \PY{n}{kernel\PYZus{}width} \PY{o}{=} \PY{p}{(}\PY{n}{max\PYZus{}val} \PY{o}{\PYZhy{}} \PY{n}{min\PYZus{}val}\PY{p}{)} \PY{o}{/} \PY{l+m+mi}{10}
              \PY{c+c1}{\PYZsh{} generate the feature values at which to estimate the density}
              \PY{n}{x\PYZus{}values} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{min\PYZus{}val}\PY{p}{,} \PY{n}{max\PYZus{}val}\PY{p}{,} \PY{n}{num}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
              \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{} extract the data by class}
              \PY{k}{for} \PY{n+nb+bp}{cls} \PY{o+ow}{in} \PY{n}{classes}\PY{p}{:}
                  \PY{c+c1}{\PYZsh{} get the feature values corresponding to the current class}
                  \PY{n}{feature\PYZus{}values} \PY{o}{=} \PY{n}{only\PYZus{}cls\PYZus{}feature\PYZus{}values}\PY{p}{(}\PY{n}{input\PYZus{}df}\PY{p}{,} \PY{n+nb+bp}{cls}\PY{p}{,} \PY{n}{feature\PYZus{}name}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} perform the kernel density estimate}
                  \PY{n}{kernel} \PY{o}{=} \PY{n}{stats}\PY{o}{.}\PY{n}{gaussian\PYZus{}kde}\PY{p}{(}\PY{n}{feature\PYZus{}values}\PY{p}{,} \PY{n}{bw\PYZus{}method}\PY{o}{=}\PY{n}{kernel\PYZus{}width}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} evaluate the density at our pre\PYZhy{}selected feature values}
                  \PY{n}{density} \PY{o}{=} \PY{n}{kernel}\PY{p}{(}\PY{n}{x\PYZus{}values}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} add a line plot of the density estimate to the current figure}
                  \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}values}\PY{p}{,} \PY{n}{density}\PY{p}{)}  \PY{c+c1}{\PYZsh{} plot the density of x\PYZus{}values}
          
          \PY{n}{feature\PYZus{}name} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sepal\PYZus{}length}\PY{l+s+s1}{\PYZsq{}}
          \PY{n}{plot\PYZus{}kde}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{n}{feature\PYZus{}name}\PY{p}{,} \PY{n}{classes}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} format the plot}
          \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{n}{feature\PYZus{}name}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{density}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{classes}\PY{p}{)}\PY{p}{;}
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{iris_lab_files/iris_lab_109_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    From these kernel density estimate plots, it seems that each class may
have a different \texttt{sepal\_length} distribution. Perhaps we can use
\texttt{sepal\_length} to help predict the correct class. At the very
least, it seems like it could be used to help distinguish iris-setosa
from iris-virginica: in most cases, if
\texttt{sepal\_length\ \textgreater{}\ 5.6} then the class is more
likely to be iris-virginica than iris-setosa.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
